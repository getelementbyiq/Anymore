{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ¬ LTX-2 Video Generation\n",
        "\n",
        "Generate videos with Lightricks' LTX-2 model (19B parameters).\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab Pro+ with A100 GPU (40GB) recommended\n",
        "- Or Colab Pro with aggressive memory optimization\n",
        "\n",
        "**Features:**\n",
        "- Text-to-Video (T2V)\n",
        "- Image-to-Video (I2V)\n",
        "- Audio generation included\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1ï¸âƒ£ Check GPU"
      ],
      "metadata": {
        "id": "check_gpu_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability and memory\n",
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"âŒ No GPU found! Go to Runtime > Change runtime type > GPU\")\n",
        "else:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"âœ… GPU: {gpu_name}\")\n",
        "    print(f\"âœ… VRAM: {gpu_mem:.1f} GB\")\n",
        "    \n",
        "    if gpu_mem < 20:\n",
        "        print(\"\\nâš ï¸ Warning: Less than 20GB VRAM. Generation may fail.\")\n",
        "        print(\"   Consider using Colab Pro+ with A100 GPU.\")\n",
        "    elif gpu_mem < 40:\n",
        "        print(\"\\nâœ… Should work with FP8 optimization and smaller videos.\")\n",
        "    else:\n",
        "        print(\"\\nâœ… Plenty of VRAM! Full quality generation possible.\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2ï¸âƒ£ Install Dependencies"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LTX-2 from GitHub\n",
        "!pip install -q uv\n",
        "\n",
        "# Clone repository\n",
        "!git clone -q https://github.com/getelementbyiq/Anymore.git /content/LTX-2\n",
        "%cd /content/LTX-2\n",
        "\n",
        "# Install dependencies\n",
        "!uv pip install --system -q -e packages/ltx-core -e packages/ltx-pipelines\n",
        "!pip install -q huggingface_hub gradio\n",
        "\n",
        "print(\"âœ… Dependencies installed!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3ï¸âƒ£ Download Models\n",
        "\n",
        "This will download ~32GB of models. Takes about 5-10 minutes."
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "MODEL_DIR = \"/content/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"ðŸ“¥ Downloading LTX-2 models...\")\n",
        "print(\"   This takes 5-10 minutes on Colab.\\n\")\n",
        "\n",
        "# Download main checkpoint (FP8 for memory efficiency)\n",
        "print(\"1/4 Downloading main checkpoint (~19GB)...\")\n",
        "hf_hub_download(\n",
        "    repo_id=\"Lightricks/LTX-2\",\n",
        "    filename=\"ltx-2-19b-distilled-fp8.safetensors\",\n",
        "    local_dir=MODEL_DIR,\n",
        "    local_dir_use_symlinks=False,\n",
        ")\n",
        "print(\"   âœ… Checkpoint downloaded\")\n",
        "\n",
        "# Download spatial upsampler\n",
        "print(\"2/4 Downloading spatial upsampler...\")\n",
        "hf_hub_download(\n",
        "    repo_id=\"Lightricks/LTX-2\",\n",
        "    filename=\"ltx-2-spatial-upscaler-x2-1.0.safetensors\",\n",
        "    local_dir=MODEL_DIR,\n",
        "    local_dir_use_symlinks=False,\n",
        ")\n",
        "print(\"   âœ… Upsampler downloaded\")\n",
        "\n",
        "# Download distilled LoRA\n",
        "print(\"3/4 Downloading distilled LoRA...\")\n",
        "hf_hub_download(\n",
        "    repo_id=\"Lightricks/LTX-2\",\n",
        "    filename=\"ltx-2-19b-distilled-lora-384.safetensors\",\n",
        "    local_dir=MODEL_DIR,\n",
        "    local_dir_use_symlinks=False,\n",
        ")\n",
        "print(\"   âœ… LoRA downloaded\")\n",
        "\n",
        "# Download Gemma text encoder\n",
        "print(\"4/4 Downloading Gemma text encoder (~12GB)...\")\n",
        "GEMMA_PATH = os.path.join(MODEL_DIR, \"gemma-3-12b-it-qat-q4_0-unquantized\")\n",
        "snapshot_download(\n",
        "    repo_id=\"google/gemma-3-12b-it-qat-q4_0-unquantized\",\n",
        "    local_dir=GEMMA_PATH,\n",
        "    local_dir_use_symlinks=False,\n",
        ")\n",
        "print(\"   âœ… Gemma downloaded\")\n",
        "\n",
        "print(\"\\nâœ… All models downloaded!\")\n",
        "\n",
        "# Show disk usage\n",
        "!du -sh /content/models/*"
      ],
      "metadata": {
        "id": "download_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4ï¸âƒ£ Load Pipeline"
      ],
      "metadata": {
        "id": "load_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Set memory optimization\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Paths\n",
        "MODEL_DIR = \"/content/models\"\n",
        "CHECKPOINT_PATH = os.path.join(MODEL_DIR, \"ltx-2-19b-distilled-fp8.safetensors\")\n",
        "UPSAMPLER_PATH = os.path.join(MODEL_DIR, \"ltx-2-spatial-upscaler-x2-1.0.safetensors\")\n",
        "LORA_PATH = os.path.join(MODEL_DIR, \"ltx-2-19b-distilled-lora-384.safetensors\")\n",
        "GEMMA_PATH = os.path.join(MODEL_DIR, \"gemma-3-12b-it-qat-q4_0-unquantized\")\n",
        "\n",
        "print(\"ðŸ”„ Loading LTX-2 pipeline...\")\n",
        "print(\"   This takes 2-3 minutes.\\n\")\n",
        "\n",
        "from ltx_core.loader import LTXV_LORA_COMFY_RENAMING_MAP, LoraPathStrengthAndSDOps\n",
        "from ltx_pipelines.distilled import DistilledPipeline\n",
        "\n",
        "# Configure distilled LoRA\n",
        "distilled_lora = [\n",
        "    LoraPathStrengthAndSDOps(\n",
        "        LORA_PATH,\n",
        "        0.8,\n",
        "        LTXV_LORA_COMFY_RENAMING_MAP\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = DistilledPipeline(\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    distilled_lora=distilled_lora,\n",
        "    spatial_upsampler_path=UPSAMPLER_PATH,\n",
        "    gemma_root=GEMMA_PATH,\n",
        "    loras=[],\n",
        "    fp8transformer=False,  # Already FP8 checkpoint\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Pipeline loaded!\")\n",
        "print(f\"   GPU Memory used: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "id": "load_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5ï¸âƒ£ Generate Video\n",
        "\n",
        "### Text-to-Video"
      ],
      "metadata": {
        "id": "generate_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ltx_pipelines.utils.media_io import encode_video\n",
        "from ltx_pipelines.utils.constants import AUDIO_SAMPLE_RATE\n",
        "from ltx_core.model.video_vae import TilingConfig, get_video_chunks_number\n",
        "from IPython.display import Video, display\n",
        "\n",
        "# ============================================\n",
        "# ðŸŽ¬ EDIT YOUR PROMPT HERE\n",
        "# ============================================\n",
        "PROMPT = \"A golden retriever running through a sunny meadow, slow motion, cinematic lighting, high quality\"\n",
        "NEGATIVE_PROMPT = \"blurry, low quality, distorted, ugly\"\n",
        "\n",
        "# Video settings\n",
        "SEED = 42              # Change for different results\n",
        "HEIGHT = 544           # 544 for 16:9, or 768 for square\n",
        "WIDTH = 960            # 960 for 16:9, or 768 for square\n",
        "NUM_FRAMES = 49        # 49 = ~2 seconds at 25fps (must be 1 + n*8)\n",
        "FRAME_RATE = 25.0\n",
        "# ============================================\n",
        "\n",
        "print(f\"ðŸŽ¬ Generating video...\")\n",
        "print(f\"   Prompt: {PROMPT[:60]}...\")\n",
        "print(f\"   Resolution: {WIDTH}x{HEIGHT}\")\n",
        "print(f\"   Frames: {NUM_FRAMES} ({NUM_FRAMES/FRAME_RATE:.1f}s)\")\n",
        "print(f\"   Seed: {SEED}\")\n",
        "print(\"\\n   This takes 1-3 minutes...\\n\")\n",
        "\n",
        "tiling_config = TilingConfig.default()\n",
        "video_chunks_number = get_video_chunks_number(NUM_FRAMES, tiling_config)\n",
        "\n",
        "# Generate\n",
        "video, audio = pipeline(\n",
        "    prompt=PROMPT,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    seed=SEED,\n",
        "    height=HEIGHT,\n",
        "    width=WIDTH,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_rate=FRAME_RATE,\n",
        "    images=[],\n",
        "    tiling_config=tiling_config,\n",
        "    enhance_prompt=False,\n",
        ")\n",
        "\n",
        "# Save video\n",
        "OUTPUT_PATH = \"/content/output.mp4\"\n",
        "encode_video(\n",
        "    video=video,\n",
        "    fps=FRAME_RATE,\n",
        "    audio=audio,\n",
        "    audio_sample_rate=AUDIO_SAMPLE_RATE,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    video_chunks_number=video_chunks_number,\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Video saved to {OUTPUT_PATH}\")\n",
        "\n",
        "# Display video\n",
        "display(Video(OUTPUT_PATH, embed=True, width=640))"
      ],
      "metadata": {
        "id": "generate_t2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image-to-Video (I2V)\n",
        "\n",
        "Upload an image and animate it!"
      ],
      "metadata": {
        "id": "i2v_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Upload image\n",
        "print(\"ðŸ“¤ Upload an image to animate:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    INPUT_IMAGE = list(uploaded.keys())[0]\n",
        "    print(f\"\\nâœ… Uploaded: {INPUT_IMAGE}\")\n",
        "    \n",
        "    # Show uploaded image\n",
        "    img = Image.open(INPUT_IMAGE)\n",
        "    display(img.resize((400, int(400 * img.height / img.width))))\n",
        "else:\n",
        "    print(\"âŒ No image uploaded\")"
      ],
      "metadata": {
        "id": "upload_image"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate video from uploaded image\n",
        "from ltx_pipelines.utils.media_io import encode_video\n",
        "from ltx_pipelines.utils.constants import AUDIO_SAMPLE_RATE\n",
        "from ltx_core.model.video_vae import TilingConfig, get_video_chunks_number\n",
        "from IPython.display import Video, display\n",
        "\n",
        "# ============================================\n",
        "# ðŸŽ¬ EDIT YOUR SETTINGS HERE\n",
        "# ============================================\n",
        "PROMPT = \"The scene comes to life with gentle movement, cinematic\"\n",
        "NEGATIVE_PROMPT = \"blurry, low quality, static, frozen\"\n",
        "SEED = 42\n",
        "NUM_FRAMES = 49\n",
        "FRAME_RATE = 25.0\n",
        "\n",
        "# Image settings\n",
        "IMAGE_FRAME_INDEX = 0   # Which frame to place the image (0 = first frame)\n",
        "IMAGE_STRENGTH = 1.0    # How strongly to condition on the image (0-1)\n",
        "# ============================================\n",
        "\n",
        "# Get image dimensions and adjust to valid sizes\n",
        "img = Image.open(INPUT_IMAGE)\n",
        "width, height = img.size\n",
        "\n",
        "# Round to nearest multiple of 32\n",
        "HEIGHT = (min(height, 544) // 32) * 32\n",
        "WIDTH = (min(width, 960) // 32) * 32\n",
        "\n",
        "print(f\"ðŸŽ¬ Generating I2V...\")\n",
        "print(f\"   Input: {INPUT_IMAGE}\")\n",
        "print(f\"   Prompt: {PROMPT[:60]}...\")\n",
        "print(f\"   Resolution: {WIDTH}x{HEIGHT}\")\n",
        "print(\"\\n   This takes 1-3 minutes...\\n\")\n",
        "\n",
        "tiling_config = TilingConfig.default()\n",
        "video_chunks_number = get_video_chunks_number(NUM_FRAMES, tiling_config)\n",
        "\n",
        "# Image conditioning\n",
        "images = [(INPUT_IMAGE, IMAGE_FRAME_INDEX, IMAGE_STRENGTH)]\n",
        "\n",
        "# Generate\n",
        "video, audio = pipeline(\n",
        "    prompt=PROMPT,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    seed=SEED,\n",
        "    height=HEIGHT,\n",
        "    width=WIDTH,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_rate=FRAME_RATE,\n",
        "    images=images,\n",
        "    tiling_config=tiling_config,\n",
        "    enhance_prompt=False,\n",
        ")\n",
        "\n",
        "# Save\n",
        "OUTPUT_PATH = \"/content/output_i2v.mp4\"\n",
        "encode_video(\n",
        "    video=video,\n",
        "    fps=FRAME_RATE,\n",
        "    audio=audio,\n",
        "    audio_sample_rate=AUDIO_SAMPLE_RATE,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    video_chunks_number=video_chunks_number,\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Video saved!\")\n",
        "display(Video(OUTPUT_PATH, embed=True, width=640))"
      ],
      "metadata": {
        "id": "generate_i2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6ï¸âƒ£ Download Video"
      ],
      "metadata": {
        "id": "download_video_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the generated video\n",
        "print(\"ðŸ“¥ Downloading video...\")\n",
        "files.download(\"/content/output.mp4\")"
      ],
      "metadata": {
        "id": "download_video"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ Tips\n",
        "\n",
        "**Prompting:**\n",
        "- Be detailed and specific\n",
        "- Describe movements, camera angles, lighting\n",
        "- Keep prompts under 200 words\n",
        "\n",
        "**Memory Issues:**\n",
        "- Reduce `NUM_FRAMES` (try 25 instead of 49)\n",
        "- Reduce resolution (try 512x512)\n",
        "- Restart runtime and try again\n",
        "\n",
        "**Quality:**\n",
        "- Higher frame counts = smoother motion\n",
        "- Try different seeds for variety\n",
        "- Use negative prompts to avoid artifacts\n",
        "\n",
        "---\n",
        "\n",
        "**Links:**\n",
        "- [LTX-2 GitHub](https://github.com/Lightricks/LTX-2)\n",
        "- [Prompting Guide](https://ltx.video/blog/how-to-prompt-for-ltx-2)"
      ],
      "metadata": {
        "id": "tips"
      }
    }
  ]
}